{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJO-t_WCtrhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAC4CNHmt1-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"gpt-2\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrdyOPItt_G4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install tensorflow-gpu==1.12.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYjFc7WIuEO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHS7vEHQuWmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 download_model.py 345M"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bouQgxKVugPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cWBumTYu9hS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('src')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEu1UCP-vC9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import model, sample, encoder"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AD7G2zkyAyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def interact_model(model_name,seed,nsamples,batch_size,length,temperature,top_k,models_dir):\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "    if batch_size is None:\n",
        "        batch_size = 1\n",
        "    assert nsamples % batch_size == 0\n",
        "\n",
        "    enc = encoder.get_encoder(model_name, models_dir)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length is None:\n",
        "        length = hparams.n_ctx // 2\n",
        "    elif length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        while True:\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "            while not raw_text:\n",
        "                print('Prompt should not be empty!')\n",
        "                raw_text = input(\"Model prompt >>> \")\n",
        "            context_tokens = enc.encode(raw_text)\n",
        "            generated = 0\n",
        "            for _ in range(nsamples // batch_size):\n",
        "                out = sess.run(output, feed_dict={\n",
        "                    context: [context_tokens for _ in range(batch_size)]\n",
        "                })[:, len(context_tokens):]\n",
        "                for i in range(batch_size):\n",
        "                    generated += 1\n",
        "                    text = enc.decode(out[i])\n",
        "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "                    print(text)\n",
        "            print(\"=\" * 80)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YvfbrAZzZjn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "b279d4c7-73d3-436e-fc1c-b9ad27854dc9"
      },
      "source": [
        "interact_model('345M',None,1,1,300,1,0,'/content/gpt-2/models')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/gpt-2/models/345M/model.ckpt\n",
            "Model prompt >>> i went to the party and\n",
            "======================================== SAMPLE 1 ========================================\n",
            " raged on about it,\" Wunten recalled. \"He warned her and her friends that he had victims and could target them.\" One of the victims drove to Polanco Burger Bar in Glen Rock and reported Sweeney immediately. Wunten drove to Polanco and mistook Polanco for a sex hook-up site Echo Italy, requested discovered provides erotic services. Cathy Steiner, the owner and manager of Polanco insisted that she and her customers choose to see if any of her customers were sex workers. Steiner claims she and her dancers grabbed producers and customers before asking themselves if they wanted to go to hell. According to witnesses Wunten said that he wanted women to retire to a forced sterilized state and lure men with youth and sex appeal. \"They told us Romanes had done this in Europe before with men,\" said Wunten. \"Yes since 14 a boy turns from one hour to over 48 hours in Romania - no punishment. Many just want a fool's Mistress to play with in their room.\"\n",
            " * * Ducklickthrough Social/*\n",
            "For more in the story, check out this graphic.\n",
            "Nancy L. Schrier, licensed clinical sex educator, doctor and culture consultant This was just the beginning Baptist funded sex included - might never get those performers were selling baby naming chicks baby names speech http://amend-healthconsulting.org/ www.cpan.org/Blog/SC/CurrentFeature/ SexAndHealth/15946edd\n",
            "================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37DFgokNzf10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}